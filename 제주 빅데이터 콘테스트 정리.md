# 제주 신용카드 빅데이터 경진대회
- 참여 기간 : 2020년 7월 8일 ~ 2020년 7월 31일
- 참여 후 제출한 주요 코드, 변경된 기능 위주 정리

## 1. 베이스라인 코드
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_bigdata_0708.ipynb"

- 점수 : 약 16.09
- 기본 순서
1. 데이터 불러오기 및 정제, 전처리
2. 데이터 인코딩
3. 데이터셋 분리, 정규화
4. 모델링(스태킹 알고리즘) 및 교차검증 수행
5. 예측 데이터셋, 예측 템플릿 생성
6. 디코딩 후 제출 파일 생성

- 스태킹 알고리즘 및 rmsle 검증
```python

def rmsle(y, pred): 
  log_y = np.log1p(y)
  log_pred = np.log1p(pred)
  squared_error = (log_y - log_pred)**2
  rmsle = np.sqrt(np.mean(squared_error))
  return print('Test Data RMSLE: {0:.3f}'.format(rmsle))
  
# 교차검증 수행
def get_best_params_model(model, params):
  cv_model = GridSearchCV(model, param_grid=params, scoring="neg_mean_squared_error", cv = 5)
  cv_model.fit(X_train, y_train)
  print("----", model.__class__.__name__, "----")
  print("GridSearchCV 최적 하이퍼 파라미터 :", cv_model.best_params_)

  rmse = np.sqrt(-1*cv_model.best_score_)
  print("GridSearchCV 최적 평균 RMSE값 :", np.round(rmse, 3))

  eval_pred = cv_model.predict(X_test)
  eval_pred = np.expm1(eval_pred)
  rmsle(y_test, eval_pred)
  
  return cv_model.best_estimator_
  
from sklearn.linear_model import LinearRegression

# 3개의 모델 선언
xgb = XGBRegressor(random_state=0)
gbm = GradientBoostingRegressor(random_state=0)
lgb = LGBMRegressor(random_state=0)  

params = {'n_estimators': [1000, 2000]}

models = [xgb, gbm, lgb] 
best_models = []
for model in models:
  new_model = get_best_params_model(model=model, params=params)
  best_models.append(new_model)
```
- 많은 모델 중 XGBoost, GBM, LightGBM 세 개를 선정, gridSearch CV를 통해 n_estimators의 최적의 값을 정함.

## 2. 지역 거주민/관광객 분리 후 훈련
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_bigdata_lcl_trst.ipynb"

- 점수 : 약 6.7
- 변경점 : 1번의 베이스라인 코드와 같이 train 데이터를 샘플링 후 그대로 진행하기 보다는 거주민과 관광객으로 나누어 각각의 소비패턴을 학습한다면 조금 더 좋은 결과를 얻을 수 있을 것이라 생각
```python
local = data[data['CARD_CCG_NM']==data['HOM_CCG_NM']].reset_index(drop=True)
tourist = data[data['CARD_CCG_NM']!=data['HOM_CCG_NM']].reset_index(drop=True)
```

## 2-1. 함수화
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_bigdata_def.ipynb"

- 2의 코드를 기능별로 함수화
- 따라서 점수의 변동은 크게 없음
- 함수가 정상적으로 작동하는지 확인하기 위해 local 부분만 함수화하여 진행
```python
def local(data):

  global local
  
  local = data[data['CARD_CCG_NM']==data['HOM_CCG_NM']].reset_index(drop=True)

  # 데이터 정제
  local = local.drop(['CARD_CCG_NM', 'HOM_CCG_NM'], axis=1)
  columns = ['CARD_SIDO_NM', 'STD_CLSS_NM', 'HOM_SIDO_NM', 'AGE', 'SEX_CTGO_CD', 'FLC', 'year', 'month']
  local = local.groupby(columns).sum().reset_index(drop=False)

  return local

def lcl_encoding(local):

  global lcl_num, encoder, encoders

  # 인코딩
  dtypes_lcl = local.dtypes
  encoders = {}
  for column in local.columns:
      if str(dtypes_lcl[column]) == 'object':
          encoder = LabelEncoder()
          encoder.fit(local[column])
          encoders[column] = encoder
          
  lcl_num = local.copy()        
  for column in encoders.keys():
      encoder = encoders[column]
      lcl_num[column] = encoder.transform(local[column])

  return lcl_num
  
def train_test(lcl_num):
  global X_train_lcl, y_train_lcl, X_test_lcl, y_test_lcl
  
  X_lcl, y_lcl = lcl_num.loc[:, lcl_num.columns != 'AMT'], lcl_num['AMT']
  X_lcl  = X_lcl.drop(['CSTMR_CNT', 'CNT'], axis=1)
  X_train_lcl, X_test_lcl, y_train_lcl, y_test_lcl = train_test_split(X_lcl, y_lcl, test_size=0.3, random_state=126, shuffle=True)
  X_train_lcl.shape, y_train_lcl.shape, X_test_lcl.shape, y_test_lcl.shape
  y_train_lcl = np.log1p(y_train_lcl)

  return X_train_lcl, y_train_lcl, X_test_lcl, y_test_lcl
```
- local 분리, 인코딩, 데이터셋 분리 외에 모델링, 예측 템플릿 생성까지의 함수 성, 정상 작동됨을 확인

## 2-2. 클래스화
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_bigdata_class.ipynb"

- 추가적으로 샘플링(이 파일에서는 파일을 빅쿼리를 통해 불러오는 기능만 작성), local/tourist 분리 함수를 작성하고, 모델링부터 예측 템플릿 생성까지의 과정을 클래스화

1) 샘플링, local/tourist 분리 함수
```python
def sampling():
    project_id = 'jeju-bigquery-282708'
    client = bigquery.Client(project=project_id)

    train = client.query('''
    SELECT 
        * 
    FROM `jeju-bigquery-282708.jeju_bigdata.201901_202003_train`
    ''').to_dataframe()

    return train

def cate(data):
    local = data[data['CARD_CCG_NM'] == data['HOM_CCG_NM']].reset_index(drop = True)
    tourist = data[data['CARD_CCG_NM'] != data['HOM_CCG_NM']].reset_index(drop = True)

    local, tourist = local.drop(['CARD_CCG_NM', 'HOM_CCG_NM'], axis=1), tourist.drop(['CARD_CCG_NM', 'HOM_CCG_NM'], axis=1)
    columns = ['CARD_SIDO_NM', 'STD_CLSS_NM', 'HOM_SIDO_NM', 'AGE', 'SEX_CTGO_CD', 'FLC', 'year', 'month']
    local, tourist = local.groupby(columns).sum().reset_index(drop=False), tourist.groupby(columns).sum().reset_index(drop=False)

    return local, tourist
```
2) 클래스
```python
class Model:
    def __init__(self, data, num):
        self.X_train = data[0]
        self.X_test = data[1]
        self.y_train = data[2]
        self.y_test = data[3]
        self.encoding_data = num

    def rmsle(self, y, pred): 
        log_y = np.log1p(y)
        log_pred = np.log1p(pred)
        squared_error = (log_y - log_pred)**2
        rmsle = np.sqrt(np.mean(squared_error))
        print(round(rmsle, 3))

        return round(rmsle, 3)

    def best_params_model(self, model, params):
        cv_model = GridSearchCV(model, param_grid=params, scoring="neg_mean_squared_error", cv = 5)
        cv_model.fit(self.X_train, self.y_train)
        eval_pred = cv_model.predict(self.X_test)
        eval_pred = np.expm1(eval_pred)
        rmsle_ = self.rmsle(self.y_test, eval_pred)

        return  cv_model.best_estimator_, rmsle_

    def get_model(self):
        xgb = XGBRegressor(random_state=0)
        gbm = GradientBoostingRegressor(random_state=0)
        lgb = LGBMRegressor(random_state=0)

        params = {'n_estimators': [1000, 2000]}

        models = [xgb, gbm, lgb]
        best_models = []
        rmsles = []
        
        for model in models:
            new_model = self.best_params_model(model, params)[0]
            new_rmsle = self.best_params_model(model, params)[1]
            best_models.append(new_model)
            rmsles.append(new_rmsle)

        self.xgb_reg = best_models[0]
        self.gbm_reg = best_models[1]
        self.lgb_reg = best_models[2]

        self.xgb_rmsle = rmsles[0]
        self.gbm_rmsle = rmsles[1]
        self.lgb_rmsle = rmsles[2]

    def final(self):

        xgb_pred = self.xgb_reg.predict(self.X_test)
        xgb_pred = np.expm1(xgb_pred)

        gbm_pred = self.gbm_reg.predict(self.X_test)
        gbm_pred = np.expm1(gbm_pred)

        lgb_pred = self.lgb_reg.predict(self.X_test)
        lgb_pred = np.expm1(lgb_pred)

        pred = np.array([xgb_pred, gbm_pred, lgb_pred])
        pred = np.transpose(pred)

        rmsle_sum = self.xgb_rmsle + self.gbm_rmsle + self.lgb_rmsle

        self.xgb_per = self.xgb_rmsle / rmsle_sum
        self.gbm_per = self.gbm_rmsle / rmsle_sum
        self.lgb_per = self.lgb_rmsle / rmsle_sum
        
        final = xgb_pred*self.xgb_per + gbm_pred*self.gbm_per + lgb_pred*self.lgb_per
        self.rmsle(self.y_test, final)

    def make_temp(self):
        CARD_SIDO_NMs = self.encoding_data['CARD_SIDO_NM'].unique()
        STD_CLSS_NMs  = self.encoding_data['STD_CLSS_NM'].unique()
        HOM_SIDO_NMs  = self.encoding_data['HOM_SIDO_NM'].unique()
        AGEs          = self.encoding_data['AGE'].unique()
        SEX_CTGO_CDs  = self.encoding_data['SEX_CTGO_CD'].unique()
        FLCs          = self.encoding_data['FLC'].unique()
        years         = [2020]
        months        = [4, 7]

        comb_list = [CARD_SIDO_NMs, STD_CLSS_NMs,HOM_SIDO_NMs, AGEs, SEX_CTGO_CDs, FLCs, years, months]
        temp = np.array(list(product(*comb_list)))

        train_features = self.encoding_data.drop(['CSTMR_CNT', 'AMT', 'CNT'], axis=1)
        tmp = pd.DataFrame(data=temp, columns=train_features.columns)

        return tmp

    def make_sub(self, temp):
        xgb_pred = self.xgb_reg.predict(temp)
        xgb_pred = np.expm1(xgb_pred)

        gbm_pred = self.gbm_reg.predict(temp)
        gbm_pred = np.expm1(gbm_pred)

        lgb_pred = self.lgb_reg.predict(temp)
        lgb_pred = np.expm1(lgb_pred)

        final_rmsle = xgb_pred*self.xgb_per + gbm_pred*self.gbm_per + lgb_pred*self.lgb_per

        temp['AMT'] = np.round(final_rmsle, 0)
        temp['REG_YYMM'] = temp['year']*100 + temp['month']
        temp = temp[['REG_YYMM', 'CARD_SIDO_NM', 'STD_CLSS_NM', 'AMT']]
        temp = temp.groupby(['REG_YYMM', 'CARD_SIDO_NM', 'STD_CLSS_NM']).sum().reset_index(drop = False)

        temp['CARD_SIDO_NM'] = encoders['CARD_SIDO_NM'].inverse_transform(temp['CARD_SIDO_NM'])
        temp['STD_CLSS_NM'] = encoders['STD_CLSS_NM'].inverse_transform(temp['STD_CLSS_NM'])

        return temp
```

## 3-1. 지역별
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_region.ipynb"

- 지역 거주민/관광객으로 나누어 진행함에 있어서 점수의 정체구간이 있었고, 조금 더 세분화하여 지역별로 나눈 후 샘플링 후에 진행하기로 결정
- 6점 대에서 머무르던 점수가 약 4.7로 크게 오름
- CARD_SIDO_NM 컬럼의 유니크한 리스트를 뽑은 후, 그 리스트로 반복문을 작성하여 각 시도별로 1만 개씩 샘플링 후 일련의 과정 진행
```python
# 시도별 예측 데이터 프레임 생성을 위한 시도 리스트 생성
sido_list = train['CARD_SIDO_NM'].unique().tolist()
sido_list

for sido in sido_list:
  temp = train[train['CARD_SIDO_NM'] == sido].reset_index(drop = True)
  temp = temp.sample(n = 10000).reset_index(drop = True)
```

- 시도별로 뽑은 데이터프레임을 하나로 병합
```python

# 시도별 예측 데이터프레임 하나로 병합
test = pd.concat([sub_강원, sub_경기, sub_경남, sub_경북, sub_광주, sub_대구, sub_대전, sub_부산, sub_서울, sub_세종, sub_울산, sub_인천, sub_전남, sub_전북, sub_제주, sub_충남, sub_충북])


# 날짜-시도별 순서로 정렬
test = test.sort_values(by = ['REG_YYMM', 'CARD_SIDO_NM']).reset_index(drop = True)
```

## 3-2. 지역별 + 증감률 반영
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_region_diff.ipynb"

- 코로나 이전인 2019년 3월과 코로나 이후인 2020년 3월 데이터의 AMT를 비교, 증감률을 구해 예측 데이터프레임에 반영
- 점수는 약 3.9로 3-1에서 조
