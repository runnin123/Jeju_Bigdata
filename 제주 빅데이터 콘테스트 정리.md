# 제주 신용카드 빅데이터 경진대회
- 참여 기간 : 2020년 7월 8일 ~ 2020년 7월 31일
- 참여 후 제출한 주요 코드, 변경된 기능 위주 정리

## 1. 베이스라인 코드
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_bigdata_0708.ipynb"

- 점수 : 약 16.09
- 기본 순서
1. 데이터 불러오기 및 정제, 전처리
2. 데이터 인코딩
3. 데이터셋 분리, 정규화
4. 모델링(스태킹 알고리즘) 및 교차검증 수행
5. 예측 데이터셋, 예측 템플릿 생성
6. 디코딩 후 제출 파일 생성

- 스태킹 알고리즘 및 rmsle 검증
```python

def rmsle(y, pred): 
  log_y = np.log1p(y)
  log_pred = np.log1p(pred)
  squared_error = (log_y - log_pred)**2
  rmsle = np.sqrt(np.mean(squared_error))
  return print('Test Data RMSLE: {0:.3f}'.format(rmsle))
  
# 교차검증 수행
def get_best_params_model(model, params):
  cv_model = GridSearchCV(model, param_grid=params, scoring="neg_mean_squared_error", cv = 5)
  cv_model.fit(X_train, y_train)
  print("----", model.__class__.__name__, "----")
  print("GridSearchCV 최적 하이퍼 파라미터 :", cv_model.best_params_)

  rmse = np.sqrt(-1*cv_model.best_score_)
  print("GridSearchCV 최적 평균 RMSE값 :", np.round(rmse, 3))

  eval_pred = cv_model.predict(X_test)
  eval_pred = np.expm1(eval_pred)
  rmsle(y_test, eval_pred)
  
  return cv_model.best_estimator_
  
from sklearn.linear_model import LinearRegression

# 3개의 모델 선언
xgb = XGBRegressor(random_state=0)
gbm = GradientBoostingRegressor(random_state=0)
lgb = LGBMRegressor(random_state=0)  

params = {'n_estimators': [1000, 2000]}

models = [xgb, gbm, lgb] 
best_models = []
for model in models:
  new_model = get_best_params_model(model=model, params=params)
  best_models.append(new_model)
```
- 많은 모델 중 XGBoost, GBM, LightGBM 세 개를 선정, gridSearch CV를 통해 n_estimators의 최적의 값을 정함.

## 2. 지역 거주민/관광객 분리 후 훈련
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_bigdata_lcl_trst.ipynb"

- 점수 : 약 6.7
- 변경점 : 1번의 베이스라인 코드와 같이 train 데이터를 샘플링 후 그대로 진행하기 보다는 거주민과 관광객으로 나누어 각각의 소비패턴을 학습한다면 조금 더 좋은 결과를 얻을 수 있을 것이라 생각
```python
local = data[data['CARD_CCG_NM']==data['HOM_CCG_NM']].reset_index(drop=True)
tourist = data[data['CARD_CCG_NM']!=data['HOM_CCG_NM']].reset_index(drop=True)
```

## 2-1. 함수화
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_bigdata_def.ipynb"

- 2의 코드를 기능별로 함수화
- 따라서 점수의 변동은 크게 없음
- 함수가 정상적으로 작동하는지 확인하기 위해 local 부분만 함수화하여 진행
```python
def local(data):

  global local
  
  local = data[data['CARD_CCG_NM']==data['HOM_CCG_NM']].reset_index(drop=True)

  # 데이터 정제
  local = local.drop(['CARD_CCG_NM', 'HOM_CCG_NM'], axis=1)
  columns = ['CARD_SIDO_NM', 'STD_CLSS_NM', 'HOM_SIDO_NM', 'AGE', 'SEX_CTGO_CD', 'FLC', 'year', 'month']
  local = local.groupby(columns).sum().reset_index(drop=False)

  return local

def lcl_encoding(local):

  global lcl_num, encoder, encoders

  # 인코딩
  dtypes_lcl = local.dtypes
  encoders = {}
  for column in local.columns:
      if str(dtypes_lcl[column]) == 'object':
          encoder = LabelEncoder()
          encoder.fit(local[column])
          encoders[column] = encoder
          
  lcl_num = local.copy()        
  for column in encoders.keys():
      encoder = encoders[column]
      lcl_num[column] = encoder.transform(local[column])

  return lcl_num
  
def train_test(lcl_num):
  global X_train_lcl, y_train_lcl, X_test_lcl, y_test_lcl
  
  X_lcl, y_lcl = lcl_num.loc[:, lcl_num.columns != 'AMT'], lcl_num['AMT']
  X_lcl  = X_lcl.drop(['CSTMR_CNT', 'CNT'], axis=1)
  X_train_lcl, X_test_lcl, y_train_lcl, y_test_lcl = train_test_split(X_lcl, y_lcl, test_size=0.3, random_state=126, shuffle=True)
  X_train_lcl.shape, y_train_lcl.shape, X_test_lcl.shape, y_test_lcl.shape
  y_train_lcl = np.log1p(y_train_lcl)

  return X_train_lcl, y_train_lcl, X_test_lcl, y_test_lcl
```
- local 분리, 인코딩, 데이터셋 분리 외에 모델링, 예측 템플릿 생성까지의 함수 성, 정상 작동됨을 확인

## 2-2. 클래스화
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_bigdata_class.ipynb"

- 추가적으로 샘플링(이 파일에서는 파일을 빅쿼리를 통해 불러오는 기능만 작성), local/tourist 분리 함수를 작성하고, 모델링부터 예측 템플릿 생성까지의 과정을 클래스화

1) 샘플링, local/tourist 분리 함수
```python
def sampling():
    project_id = 'jeju-bigquery-282708'
    client = bigquery.Client(project=project_id)

    train = client.query('''
    SELECT 
        * 
    FROM `jeju-bigquery-282708.jeju_bigdata.201901_202003_train`
    ''').to_dataframe()

    return train

def cate(data):
    local = data[data['CARD_CCG_NM'] == data['HOM_CCG_NM']].reset_index(drop = True)
    tourist = data[data['CARD_CCG_NM'] != data['HOM_CCG_NM']].reset_index(drop = True)

    local, tourist = local.drop(['CARD_CCG_NM', 'HOM_CCG_NM'], axis=1), tourist.drop(['CARD_CCG_NM', 'HOM_CCG_NM'], axis=1)
    columns = ['CARD_SIDO_NM', 'STD_CLSS_NM', 'HOM_SIDO_NM', 'AGE', 'SEX_CTGO_CD', 'FLC', 'year', 'month']
    local, tourist = local.groupby(columns).sum().reset_index(drop=False), tourist.groupby(columns).sum().reset_index(drop=False)

    return local, tourist
```
2) 클래스
```python
class Model:
    def __init__(self, data, num):
        self.X_train = data[0]
        self.X_test = data[1]
        self.y_train = data[2]
        self.y_test = data[3]
        self.encoding_data = num

    def rmsle(self, y, pred): 
        log_y = np.log1p(y)
        log_pred = np.log1p(pred)
        squared_error = (log_y - log_pred)**2
        rmsle = np.sqrt(np.mean(squared_error))
        print(round(rmsle, 3))

        return round(rmsle, 3)

    def best_params_model(self, model, params):
        cv_model = GridSearchCV(model, param_grid=params, scoring="neg_mean_squared_error", cv = 5)
        cv_model.fit(self.X_train, self.y_train)
        eval_pred = cv_model.predict(self.X_test)
        eval_pred = np.expm1(eval_pred)
        rmsle_ = self.rmsle(self.y_test, eval_pred)

        return  cv_model.best_estimator_, rmsle_

    def get_model(self):
        xgb = XGBRegressor(random_state=0)
        gbm = GradientBoostingRegressor(random_state=0)
        lgb = LGBMRegressor(random_state=0)

        params = {'n_estimators': [1000, 2000]}

        models = [xgb, gbm, lgb]
        best_models = []
        rmsles = []
        
        for model in models:
            new_model = self.best_params_model(model, params)[0]
            new_rmsle = self.best_params_model(model, params)[1]
            best_models.append(new_model)
            rmsles.append(new_rmsle)

        self.xgb_reg = best_models[0]
        self.gbm_reg = best_models[1]
        self.lgb_reg = best_models[2]

        self.xgb_rmsle = rmsles[0]
        self.gbm_rmsle = rmsles[1]
        self.lgb_rmsle = rmsles[2]

    def final(self):

        xgb_pred = self.xgb_reg.predict(self.X_test)
        xgb_pred = np.expm1(xgb_pred)

        gbm_pred = self.gbm_reg.predict(self.X_test)
        gbm_pred = np.expm1(gbm_pred)

        lgb_pred = self.lgb_reg.predict(self.X_test)
        lgb_pred = np.expm1(lgb_pred)

        pred = np.array([xgb_pred, gbm_pred, lgb_pred])
        pred = np.transpose(pred)

        rmsle_sum = self.xgb_rmsle + self.gbm_rmsle + self.lgb_rmsle

        self.xgb_per = self.xgb_rmsle / rmsle_sum
        self.gbm_per = self.gbm_rmsle / rmsle_sum
        self.lgb_per = self.lgb_rmsle / rmsle_sum
        
        final = xgb_pred*self.xgb_per + gbm_pred*self.gbm_per + lgb_pred*self.lgb_per
        self.rmsle(self.y_test, final)

    def make_temp(self):
        CARD_SIDO_NMs = self.encoding_data['CARD_SIDO_NM'].unique()
        STD_CLSS_NMs  = self.encoding_data['STD_CLSS_NM'].unique()
        HOM_SIDO_NMs  = self.encoding_data['HOM_SIDO_NM'].unique()
        AGEs          = self.encoding_data['AGE'].unique()
        SEX_CTGO_CDs  = self.encoding_data['SEX_CTGO_CD'].unique()
        FLCs          = self.encoding_data['FLC'].unique()
        years         = [2020]
        months        = [4, 7]

        comb_list = [CARD_SIDO_NMs, STD_CLSS_NMs,HOM_SIDO_NMs, AGEs, SEX_CTGO_CDs, FLCs, years, months]
        temp = np.array(list(product(*comb_list)))

        train_features = self.encoding_data.drop(['CSTMR_CNT', 'AMT', 'CNT'], axis=1)
        tmp = pd.DataFrame(data=temp, columns=train_features.columns)

        return tmp

    def make_sub(self, temp):
        xgb_pred = self.xgb_reg.predict(temp)
        xgb_pred = np.expm1(xgb_pred)

        gbm_pred = self.gbm_reg.predict(temp)
        gbm_pred = np.expm1(gbm_pred)

        lgb_pred = self.lgb_reg.predict(temp)
        lgb_pred = np.expm1(lgb_pred)

        final_rmsle = xgb_pred*self.xgb_per + gbm_pred*self.gbm_per + lgb_pred*self.lgb_per

        temp['AMT'] = np.round(final_rmsle, 0)
        temp['REG_YYMM'] = temp['year']*100 + temp['month']
        temp = temp[['REG_YYMM', 'CARD_SIDO_NM', 'STD_CLSS_NM', 'AMT']]
        temp = temp.groupby(['REG_YYMM', 'CARD_SIDO_NM', 'STD_CLSS_NM']).sum().reset_index(drop = False)

        temp['CARD_SIDO_NM'] = encoders['CARD_SIDO_NM'].inverse_transform(temp['CARD_SIDO_NM'])
        temp['STD_CLSS_NM'] = encoders['STD_CLSS_NM'].inverse_transform(temp['STD_CLSS_NM'])

        return temp
```

## 3-1. 지역별
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_region.ipynb"

- 지역 거주민/관광객으로 나누어 진행함에 있어서 점수의 정체구간이 있었고, 거주민/관광객과 같은 이분법보다는 지역별로 나눈 후, 샘플링 진행하기로 결정
- 6점 대에서 머무르던 점수가 약 4.7로 크게 오름
- CARD_SIDO_NM 컬럼의 유니크한 리스트를 뽑은 후, 그 리스트로 반복문을 작성하여 각 시도별로 1만 개씩 샘플링 후 일련의 과정 진행
```python
# 시도별 예측 데이터 프레임 생성을 위한 시도 리스트 생성
sido_list = train['CARD_SIDO_NM'].unique().tolist()
sido_list

for sido in sido_list:
  temp = train[train['CARD_SIDO_NM'] == sido].reset_index(drop = True)
  temp = temp.sample(n = 10000).reset_index(drop = True)
```

- 시도별로 뽑은 데이터프레임을 하나로 병합
```python

# 시도별 예측 데이터프레임 하나로 병합
test = pd.concat([sub_강원, sub_경기, sub_경남, sub_경북, sub_광주, sub_대구, sub_대전, sub_부산, sub_서울, sub_세종, sub_울산, sub_인천, sub_전남, sub_전북, sub_제주, sub_충남, sub_충북])


# 날짜-시도별 순서로 정렬
test = test.sort_values(by = ['REG_YYMM', 'CARD_SIDO_NM']).reset_index(drop = True)
```

## 3-2. 지역별 + 증감률 반영
"https://github.com/runnin123/Jeju_Bigdata/blob/master/Jeju_region_diff.ipynb"

- 코로나 이전인 2019년 3월과 코로나 이후인 2020년 3월 데이터의 AMT를 비교, 증감률을 구해 예측 데이터프레임에 반영(훈련 데이터는 대다수(201901~202002)가 코로나 영향을 받지 않은 데이터였기에 코로나 영향을 변수로 생성해서 반영하기 이전에 수치 적용을 하려함)
- 점수는 약 3.9로 3-1에서 조금 상승했음을 확인
```python
# 시도별 2019년 3월 AMT와 2020년 3월 AMT를 비교, 증감율 column을 생성하는 함수

def sido_diff(sido):
  df = train[train['CARD_SIDO_NM'] == sido]
  df_19 = df[df['REG_YYMM'] == 201903]
  df_20 = df[df['REG_YYMM'] == 202003]
  diff = pd.concat([df_19.groupby('STD_CLSS_NM').AMT.sum(), df_20.groupby('STD_CLSS_NM').AMT.sum()], axis=1, keys = ['1903_AMT', '2003_AMT']).reset_index()
  diff = diff.fillna(0)
  diff['diff'] = (diff['1903_AMT']-diff['2003_AMT'])/diff['1903_AMT']
  diff.columns = ['STD_CLSS_NM', '1903_AMT', '2003_AMT', 'diff']
  diff[diff['1903_AMT'] == 0]['diff'] = 0
  diff = diff[['STD_CLSS_NM', 'diff']]
  diff = diff.sort_values(by='diff', ascending=False).reset_index(drop=True)

  return diff
  
# for문 이용, 각 시도별 증감율 데이터프레임 생성
for sido in sido_list:
  tmp = sido_diff(sido)

  globals()['diff_{}'.format(sido)] = tmp
  
# subs : 시도별 예측 데이터프레임 리스트/diffs : 시도별 증감률 데이터프레임 리스트
for i in range(len(sido_list)):
  subs[i] = subs[i].merge(diffs[i][['STD_CLSS_NM','diff']], how='left', on='STD_CLSS_NM')
  subs[i]['AMT'] = subs[i]['AMT']*(1-subs[i]['diff'])
  subs[i] = subs[i][['REG_YYMM','CARD_SIDO_NM','STD_CLSS_NM','AMT']]
```

## 4. 추가 변수 생성
### 4-1. 3월, 4월의 실제 데이터 비교, 회복 추세 여부 변수 생성
"https://github.com/runnin123/Jeju_Bigdata/blob/master/jeju_diff.ipynb"

```python
data_2004 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/jeju/202004.csv')
data_2003 = train[train['REG_YYMM'] == 202003]

tmp03 = data_2003.groupby(['CARD_SIDO_NM', 'STD_CLSS_NM']).AMT.sum().reset_index()
tmp04 = data_2004.groupby(['CARD_SIDO_NM', 'STD_CLSS_NM']).AMT.sum().reset_index()
tmp = pd.merge(tmp03, tmp04, on = ['CARD_SIDO_NM', 'STD_CLSS_NM'])
tmp.columns = ['CARD_SIDO_NM', 'STD_CLSS_NM', 'AMT_03', 'AMT_04']
tmp

tmp['num'] = tmp.apply(lambda x: 1 if x['AMT_03'] < x['AMT_04'] else 0, axis = 1)
tmp

data = data.merge(tmp[['CARD_SIDO_NM', 'STD_CLSS_NM', 'num']], how = 'left')

data
```
### 4-2. 회복 추세 변수(4-1) + 업종 카테고리 분류
"https://github.com/runnin123/Jeju_Bigdata/blob/master/jeju_diff_cate.ipynb"

- 업종의 특성대로 분류를 했으나 결과값이 그리 좋게 나오지 않아, 매출 변화 그래프를 바탕으로 상승(+1), 감소(-1), 큰 변동 없음(0) 등 세 분류로 나눔. 이는 마지막에 좀 더 세부적으로 분할해서 보충함.
```python
def grap_cate(data):
  incr = ['골프장 운영업']
  des = ['관광 민예품 및 선물용품 소매업', '그외 기타 스포츠시설 운영업', '마사지업', '면세점', '버스 운송업', '여관업', '욕탕업', '전시 및 행사 대행업', '정기 항공 운송업', '호텔업', '휴양콘도 운영업', '여행사업' '일반유흥 주점업']

  if data in incr:
    return 1
  elif data in des:
    return -1
  else:
    return 0

# 기존 데이터 정제 코드에 'cate' 컬럼 생성 코드 작성
def data_pre(data):
  data = data.fillna('')
  data['year'] = data['REG_YYMM'].apply(lambda x: grap_year(x))
  data['month'] = data['REG_YYMM'].apply(lambda x: grap_month(x))
  data['cate'] = data['STD_CLSS_NM'].apply(lambda x: grap_cate(x))
  data = data.drop(['REG_YYMM'], axis = 1)

  return data
```

### 4-3. 회복 추세 변수(4-1) + 코로나 발생 변수 생성(202003, 202004)
"https://github.com/runnin123/Jeju_Bigdata/blob/master/jeju_diff_corona.ipynb"

- 202003 데이터와 202004 데이터에 코로나 변수를 넣어준다면 학습 과정에서 코로나 이전과 이후의 데이터를 조금 더 수월하게 구분할 것이라 판단.
```python
def grap_corona(data):
  corona = [202002, 202003]
  
  if data in corona:
    return 1
  else :
    return 0
 
# 마찬가지로 'corona' 컬럼을 생성하는 코드 작성
def data_pre(data):
  data = data.fillna('')
  data['year'] = data['REG_YYMM'].apply(lambda x: grap_year(x))
  data['month'] = data['REG_YYMM'].apply(lambda x: grap_month(x))
  data['corona'] = data['REG_YYMM'].apply(lambda x: grap_corona(x))
  data = data.drop(['REG_YYMM'], axis = 1)

  return data
```

## 5. 최종 제출 코드
"https://github.com/runnin123/Jeju_Bigdata/blob/master/jeju_final.ipynb"

- 상기된 요소들을 모두 포함(지역별 학습, 증감률, 회복 추세 변수, 업종 카테고리, 코로나 변수)
- 다만 4-2의 업종 카테고리 변수를 조금 더 보완하기 위해서 2020년 2월 ~ 2020년 4월의 시도별, 업종별 소비패턴 그래프를 확인 후, 작성한 엑셀 파일을 불러와 변수 생성
```python
std_num = pd.read_excel('/content/drive/My Drive/Colab Notebooks/jeju/소비패턴.xlsx', header = 1)
std_num.rename(columns = {'지역' : 'STD_CLSS_NM'}, inplace = True)

# 메인 코드에 컬럼명을 바꿔주는 코드 작성
temp.rename(columns = {sido : 'num_2020'}, inplace = True)
```
- 마지막으로, 제주 빅데이터 콘테스트의 최종 목표는 기존에 채점되던 2020년 4월 데이터가 아닌, 2020년 7월 데이터를 예측하는 것이므로, 최종 제출 버전에는 2019년 7월 데이터를 기반으로 한 결측치 대체 과정을 실시함.
- 지역 경제 보고서 등을 확인해본 결과, 대체로 회복 추세를 보이고 있으나, 작년 7월 데이터와 거의 같은 수준까지 회복하진 않을 것이라 판단하여 증감률은 그대로 반영을 해주었음. 
```python
train07 = train[train['REG_YYMM'] == 201907]
train07 = train07[['REG_YYMM', 'CARD_SIDO_NM', 'STD_CLSS_NM', 'AMT']]
train07

train07 = train07.groupby(['REG_YYMM','CARD_SIDO_NM','STD_CLSS_NM']).AMT.sum().reset_index()

for i in range(len(sido_list)):
  tmp1 = train07[train07['CARD_SIDO_NM'] == sido_list[i]].merge(diffs[i], on = 'STD_CLSS_NM')
  tmp1['pred'] = tmp1['AMT']*(1-tmp1['diff'])
  globals()[sido_list[i]] = nan[nan['CARD_SIDO_NM'] == sido_list[i]].merge(tmp1[['STD_CLSS_NM', 'pred']], on = 'STD_CLSS_NM', how = 'left')
```

## 6. 결론
- 최종 제출 스코어는 2.67 정도로 마무리하였으나, 이는 4월 데이터를 채점한 결과이므로 최종 스코어가 어떻게 될 지는 결과가 나오기 전까지는 알 수가 없음. 다만 마지막에 삽입한 7월 데이터를 기반으로 한 결측치 대체가 점수 감소로 이어지지는 않았기에, 7월 예측 역시도 크게 어긋나지 않는 선에서 이루어질 것이라 생각.
- 최종 제출이 끝난 시점에서 아쉬운 점을 꼽아보자면,
1. 제주도의 채점 가중치가 3배이나, 제주도에 조금 더 집중하지 못했던 점.
2. 예측치와 실제값(2020년 4월 기준)의 차이를 비교하였을 때, 시도별 데이터가 클 수록(1위 서울), 업종별 AMT의 합이 클 수록(슈퍼마켓, 한식점, 편의점 등) 그 오차가 꽤 크게 남. 서울에 비해 세종, 제주는 오차가 거의 없었고 업종에 있어서도 상기한 상위 업종에 비해 하위 업종은 오차가 미미하게 발생. 이 부분을 처리하지 못한 점이 다소 아쉬움.
- 하지만 의논 과정에서 나온 아이디어를 반영할 때마다 점수가 상승함을 확인했고, 향후 경험하게 될 데이터 분석에 있어서도 조금 더 자신감있게 접근할 수 있을거라 생각함.
